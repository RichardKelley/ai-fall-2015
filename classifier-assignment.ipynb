{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier Using Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By completing this notebook, you should get a good sense of how to use a gradient-based method to train a classifier. We're going to build modules for linear transformations of data, componentwise applications of the \"sigmoid\" function to a vector, and the mean-squared error loss. Once we have these modules, we will combine them to create a logistic regression classifier, which we will train on some randomly-generated data to see how the training process alters the decision-making process that the classifier goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear` module computes the transformation $f(x,W,b) = Wx + b$ for a given input $x$ and parameters $W$ and $b$. The main goal of \"training\" our classifier will be to move from the randomly initialized $W$ and $b$ that we get by instantiating a module to the \"best\" $W$ and $b$ for a given data set.\n",
    "\n",
    "The `__init__` method should take an input dimension `n_in` and an output dimension `n_out`, and should create the following instance variables:\n",
    "\n",
    "1. The `n_out` by `n_in` matrix $W$.\n",
    "2. The `n_out` vector $b$.\n",
    "3. The `n_out` vector `output`, representing the output of the module.\n",
    "4. The `n_in` vector `gradInput`, representing $\\frac{\\partial L}{\\partial x}$.\n",
    "5. The `n_out` by `n_in` matrix `gradW`, representing $\\frac{\\partial L}{\\partial W}$.\n",
    "6. The `n_out` vector `gradb`, representing $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "The matrix $W$ and vector $b$ should be initialized randomly. The other objects can be initialized to arrays of the correct shape filled with zeros.\n",
    "\n",
    "The `updateOutput` function should update the value of the `output` variable and should return that `output` value.\n",
    "\n",
    "The `updateGradInput` function should update the `gradInput` variable and return the new value of `gradInput`.\n",
    "\n",
    "The `updateGradWeight` function should update _both_ `gradW` and `gradb` and should return a tuple consisting of `gradW` and `gradb`.\n",
    "\n",
    "In case you're rusty on your chain rule, recall that \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = W^T \\frac{\\partial L}{\\partial y}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} x^T$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def updateGradWeight(self, input, gradOutput):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sigmoid` module will be responsible for calculating $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$. Since there are no tunable parameters to this module, we only have to implement `updateOutput` and `updateGradInput` here.\n",
    "\n",
    "The `__init__` function takes an integer size parameter (the number of dimensions of the input) and should create two member variables:\n",
    "\n",
    "1. An `output` variable which represents the output of the module.\n",
    "2. A `gradInput` variable which represents $\\frac{\\partial L}{\\partial x}$.\n",
    "\n",
    "The `updateOutput` function should compute the output of the module, store that in the `output` member variable, and return that value.\n",
    "\n",
    "The `updateGradInput` function should update the `gradInput` member variable and return that value.  \n",
    "\n",
    "Remember that we have the following identity:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} * \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(object):\n",
    "    def __init__(self, n_in):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate our performance and perform training, we will use a loss function based on the \"mean squared error\". For a model output $y$ and known correct answer $t$ (both vectors in some finite-dimensional vector space $\\mathbb{R}^n$), this loss can be can be computed as\n",
    "\n",
    "$$ L(y,t) = \\frac{1}{n} \\sum_{i=1}^n (y_i - t_i)^2 $$\n",
    "\n",
    "Your module's `__init__` function should create the following values:\n",
    "\n",
    "1. An `output` variable, initialized to some reasonable scalar value (like 0.0).\n",
    "2. A `gradOutput` variable, which should be an `ndarray` of size $n$ (the number of dimensions of the input vector).\n",
    "\n",
    "The `updateOutput` function should accept two arguments: `input` is the value produced by your model, and `target` is the known correct value corresponding to the model's input. The `updateOutput` argument should produce the error obtained for `input` and `target`. This will be a scalar value.\n",
    "\n",
    "The `updateGradInput` function should calculate $\\frac{\\partial L}{\\partial y}$ using the following relation:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial y} = \\frac{2}{n} * (y - t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSELoss(object):\n",
    "    def __init__(self, n_in):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def updateOutput(self, input, target):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def updateGradInput(self, input, target):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the `Classifier` object is to create an object that links together the modules defined above for the purposes of classification and training.\n",
    "\n",
    "The `__init__` function should take an integer `n_in` giving the number of dimensions of an input point. The `__init__` function should create member variables for a `Linear` module, a `Sigmoid` module, and a `MSELoss` module, all with the approriate dimensions.\n",
    "\n",
    "The `forward` function should accept an input that is to be classified, and optionally a `target` argument. If there is no target, the `forward` function should just return the result of the classifier's `Sigmoid` module. If there is a `target` argument, the `forward` function should call the `updateOutput` function of the classifier's `MSELoss` before returning the output of the `Sigmoid` module.\n",
    "\n",
    "The `backward` function should take an `input`, the corresponding `target`, and should propagate derivatives backwards through the model to obtain $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$. The `backward` function should return a tuple whose components are those derivatives.\n",
    "\n",
    "The `predict` function should take an `input` argument and should produce a label for that point. The resulting label should be either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__(self, n_in):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input, target = None):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def backward(self, input, target):\n",
    "        # Assumes forward has already been called with input\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def predict(self, input):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if our system learns anything, we're going to do a 2D classification problem (just like kNN). The next cell generates and visualizes the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(easy_x, easy_y) = datasets.make_classification(n_samples=400, n_features = 2, n_informative = 2,\n",
    "                             n_redundant = 0, n_repeated = 0, n_clusters_per_class=1, class_sep=2)\n",
    "scatter(easy_x[:,0], easy_x[:,1], c = easy_y, cmap = 'cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing a classification problem in the plane, the next line should produce exactly what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows what an untrained classifier does. Unless you happen to get lucky, you should see no correlation between the line drawn by your classifier and the distribution of points.\n",
    "\n",
    "We also print out the $W$ and $b$ parameters for our model. These will be modified by training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = 0.05 # mesh size\n",
    "x_min, x_max = easy_x[:, 0].min() - 1, easy_x[:,0].max() + 1\n",
    "y_min, y_max = easy_x[:, 1].min() - 1, easy_x[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = []\n",
    "for pt in np.c_[xx.ravel(), yy.ravel()]:\n",
    "    Z.append(classifier.predict(pt))\n",
    "Z = np.asarray(Z)\n",
    "Z = Z.reshape(xx.shape)\n",
    "figure()\n",
    "pcolormesh(xx,yy, Z, cmap='cool')\n",
    "\n",
    "print classifier.linear.W, classifier.linear.b\n",
    "\n",
    "scatter(easy_x[:,0], easy_x[:,1], c=easy_y, cmap='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training should be the following:\n",
    "\n",
    "- Loop over the entire dataset $k$ times (say, 1000). On each iteration:\n",
    "- Loop over each point in the dataset. For a given point in the dataset (consisting of an input $x$ and a label $y$):\n",
    "- Run the classifier's `forward` method to compute an error for the given $(x,y)$ pair.\n",
    "- Then run the classifier's `backward` method to get the necessary gradients.\n",
    "- Then update the model parameters using the gradient descent update rule described in class. On iteration $k+1$, the update rules you should use are\n",
    "\n",
    "  $$ W^{(k+1)} \\leftarrow W^{(k)} - \\alpha_W * \\frac{\\partial L}{\\partial W} $$ \n",
    "  \n",
    "  and\n",
    "\n",
    "  $$ b^{(k+1)} \\leftarrow b^{(k)} - \\alpha_b * \\frac{\\partial L}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "for i in range(1000):\n",
    "    for j in range(easy_x.shape[0]):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once properly trained, the decision line determined by your model should \"line up\" correctly with your data, so we re-run the mesh visualization to see how we do. We will learn about quantifying our error later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = 0.05 # mesh size\n",
    "x_min, x_max = easy_x[:, 0].min() - 1, easy_x[:,0].max() + 1\n",
    "y_min, y_max = easy_x[:, 1].min() - 1, easy_x[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = []\n",
    "for pt in np.c_[xx.ravel(), yy.ravel()]:\n",
    "    Z.append(classifier.predict(pt))\n",
    "Z = np.asarray(Z)\n",
    "Z = Z.reshape(xx.shape)\n",
    "figure()\n",
    "pcolormesh(xx,yy, Z, cmap='cool')\n",
    "\n",
    "print classifier.linear.W, classifier.linear.b\n",
    "\n",
    "scatter(easy_x[:,0], easy_x[:,1], c=easy_y, cmap='cool')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
